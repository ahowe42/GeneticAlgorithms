{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0669b26f-da8b-4693-9c22-f8115aeb8864",
   "metadata": {},
   "source": [
    "# Genetic Algorithms\n",
    "## Evolutionary Algorithms for Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820ed8c3-0a04-464a-9e70-55c9f91125fa",
   "metadata": {},
   "source": [
    "### Mathematical Optimization\n",
    "Optimization is the process of finding some numerical values that generate a minimum or maximum value for a specified function. Examples include finding the best parameters for a statistical distribution fit to some data, numerically solving differential equations, or feature selection in machine learning.\n",
    "\n",
    "There are many types and classes of optimization algorithms. Most that are invented by mathematicianas and computer scientists rely on function derivatives / gradients. Anybody who's studied the topic even slightly will likely remember [Newton-Raphson](https://en.wikipedia.org/wiki/Newton%27s_method) or the [BFGS](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm) - the latter of which even need the second derivaties (the [Hessian](https://en.wikipedia.org/wiki/Hessian_matrix)). Mathematical optimization algorithms typically iterate over two steps:\n",
    "\n",
    "1.  evaluate current solution\n",
    "2. find new solution to try\n",
    "\n",
    "A series of individual solutions are identified and evluated, until the sequence converges to a solution.\n",
    "\n",
    "Gradient-following methods can have good properties, but also two major issues.\n",
    "\n",
    "1. The first is the need to compute derivatives. This can often be very difficult - even assuming derivatives can be analytically solved - and time consuming. Furthermore, not all functions we wish to optimizte even have a derivative. What is the derivative of an accuracy loss function for a [Decision Tree Classifier](https://en.wikipedia.org/wiki/Decision_tree_learning)?\n",
    "\n",
    "2. The second issue is that gradient-followers can easily get stuck in local optima, rather than finding a global optimum point. Most such algorithms rely on being provided an initial solution, at which point the necessary derivatives are computed to determine the direction (and perhaps distance) to find the next point to evaluate. Depending on the curvature of the function to optimize, and the initial solution, the algorithm may get stuck in a local, not global, optimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "703b3508-3de3-43b0-a8f9-1b9fed5cb893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOOO: demo finding local not global optimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3600b6c-f257-4268-8d85-0265dba18e9e",
   "metadata": {},
   "source": [
    "### Non-Gradent Following Optimization Algorithms\n",
    "\n",
    "There are several types of mathematical optimization algorithms which similarly operate on a sequence of individual solutions, but don't use derivatives. I am familiar with [Simulated Annealing](https://en.wikipedia.org/wiki/Simulated_annealing) and [Golden Section Search](https://en.wikipedia.org/wiki/Golden-section_search) opimizatin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb67cf5-2e9b-4f91-aadb-4b63a7afd19e",
   "metadata": {},
   "source": [
    "### Evolutionary Algorithms\n",
    "The term [Evolutionary Algorithm (EA)](https://en.wikipedia.org/wiki/Evolutionary_algorithm) refers to a population-based metaheuristic optimization algorithm, and is a subset of evolutionary computation. Broadly, evolutionary algorithms are designed around concepts which come from biological evolution. There are several classes of evolutionary algorithms:\n",
    "\n",
    "- Differential Evolution\n",
    "- Evolutionary Programming\n",
    "- Evolutionary Strategy\n",
    "- Genetic Algorithm\n",
    "- Genetic Programming ([see here](https://github.com/ahowe42/baseball))\n",
    "- Learning Classifier System\n",
    "- Neuroevolution\n",
    "\n",
    "This set of lectures is focused on the Genetic Algorithm (GA), but could potentially be extended to include [Genetic Programming](https://en.wikipedia.org/wiki/Genetic_programming)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04162e2f-4de0-46aa-89a7-38967b81f865",
   "metadata": {},
   "source": [
    "### Other Gradient Eschewing Algorithms\n",
    "In addition to EA's, there are several other types of metaheuristic optimization algorithms that are based on the idea of optimizing with a population of potential solutions. These include:\n",
    "\n",
    "- [Ant Colony Optimization - or Traveling Ant Colony Optimization (TACO)](https://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms)\n",
    "- [Particle Swarm Optimization](https://en.wikipedia.org/wiki/Particle_swarm_optimization)\n",
    "- [Bees Algorithm](https://en.wikipedia.org/wiki/Bees_algorithm)\n",
    "- [Adaptive Dimensional Search](https://en.wikipedia.org/wiki/Adaptive_dimensional_search)\n",
    "- [Gaussian Adaptation](https://en.wikipedia.org/wiki/Gaussian_adaptation)\n",
    "- Harmony Search - special case of [Evolution Strategy](https://en.wikipedia.org/wiki/Evolution_strategy)\n",
    "\n",
    "I am only familiar with a few of these."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a21a764-3342-4eae-8f71-4df9bba23069",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
